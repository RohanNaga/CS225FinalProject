## Leading Question 
We hope to learn how patents refer and build on each other. Using this data we will analyze and create conclusions based on the number of patents that refer to each other to identify fundamental patents and innovations in each industry. The question we are solving is: What are the most fundamental patents in each industry that created core innovations and which patents created secondary innovations? We define “the most fundamental patent” as the patent that has the citations to it. In other words, the most fundamental patent is simply the patent that has been cited the most number of times (the node/patent with most in-going edges). This means that this patent has come up with a principle that is important or underlying to the creation of other patents. We will achieve this through Prim’s Algorithm (covered), Eulerian path/cycle identification (uncovered), Depth first search (traversal) and the data acquisition and format methods listed below. Our algorithm listed below is what will allow us to make our final conclusions.

## Dataset Acquisition
We are using the Stanford Network Analysis Platform (SNAP) patent citation network dataset. This dataset contains all patents published from 1963 to 1999 and the patents that a specific patent references in the patent writeup. This dataset is publicly available on the SNAP website and will be used for our algorithm. 

## Data Format
The dataset that we are using is massive. It contains over 3.9 million entries in this dataset. Running through 3.9 million entries will likely lead to incredibly high runtime, so instead, we will only use the first 10000 entries. Although this will cause bias, it will eliminate many future issues with the runtime of the program. The data is organized in a way that the first value in the entry is the id of the patent and the second entry is the id of the patent the first patent refers to (in the cit-Patents.txt.gz file). The source of the dataset is the United States Patent Office. We will use the data organized by the first patent connecting to the next one to create our original graph.

## Data Correction
Using this data set, we will retrieve connected components of a graph. In order to ensure that the input data is parsed correctly, we will create a small graph with approximately 20 nodes/edges of known patents and corresponding patent references. Using this smaller scaled graph, we will write Catch2 test cases to look for any placement errors while parsing or any connection errors between patent nodes. First, we will use Prim’s algorithm to check that all the nodes are connected in one connected component. We will then take a minimum spanning tree and run the DFS algorithm on it to create an adjacency matrix for the connected component. Once this is ensured, we will expand and run this algorithm for all 5000 nodes. 

## Data Storage
We will be using a graph to represent the dataset as each individual patent as a node and a directed edge from that node to another node showing that the node that edge starts from is the original patent and the other patent that the directed edge goes to is the cited patent. We will then perform a Prim's algorithm on the graph to get a minimum spanning tree which we will perform a Depth first search on to create an adjacency matrix for the nodes we are analyzing which were in the minimum spanning tree. We will then use a Depth first search on a new graph  of the connected component to fill in the adjacency matrix with a 1 to represent a directed edge from one node to another and a 0 for no directed connection between the two nodes. We will then sum the columns to see how many references each patent has. The highest sum of a column is the patent with the most number of references to it. Each row and column represents an individual node (patent). We are aiming for our total storage cost for our dataset after taking into account our approach to be in O(n^2).

## Algorithm 
First, we will be using Prim’s algorithm with input of the patent nodes, and the expected outcome would be the minimum spanning tree that highlights the adjacent nodes (patents) of that particular node (patent). The runtime of this algorithm would be O(n^2) where n is the number of nodes. From this, we will perform DFS to create and populate the adjacency matrix which we will use to find the most fundamental patent (with the most in-going edges). The runtime for this DFS traversal would be O(n) and the runtime to populate the adjacency matrix would be O(n^2). We will also be using the Eulerian path/cycle identification algorithm to ensure that the connected components are not cyclic given the input of the graph node. The runtime would be O(E) where E is the number of edges.

## Timeline
Here is the timeline for this project: 
Data Acquisition: November 11th
Data Processing: November 18th
Depth-First Search Completion: November 25th
Max Heapify Function Completion: November 30th
Final Project Submission: December 4th
